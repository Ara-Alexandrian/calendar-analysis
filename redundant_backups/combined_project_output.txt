--- START OF FILE: _code-concatenator.py ---
import os
import sys

# --- Configuration ---
PROJECT_ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) # Assumes script is saved in project root
OUTPUT_FILENAME = "combined_project_output.txt" # File to save the output

# Directories to exclude completely
EXCLUDE_DIRS = {
    "__pycache__",
    ".git",
    ".vscode",
    ".idea",
    "venv", 
    "env", 
    ".env", 
    "output", 
}

# File extensions or specific filenames to exclude
EXCLUDE_FILES = {
    ".pyc",
    ".DS_Store",
    OUTPUT_FILENAME, # Don't include the output file itself
}

# Extensions for files to sample (first 100 lines)
SAMPLE_EXTENSIONS = {
    ".json",
    ".txt",
    ".md",
    ".yaml",
    ".yml",
    ".csv",
    ".env", 
    ".ics", 
}

MAX_SAMPLE_LINES = 100
# --- End Configuration ---

def combine_files(root_dir):
    """Walks through the directory and combines file contents."""
    combined_content = []
    processed_files = set()

    for dirpath, dirnames, filenames in os.walk(root_dir, topdown=True):
        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]

        for filename in filenames:
            file_ext = os.path.splitext(filename)[1].lower()
            if filename in EXCLUDE_FILES or file_ext in EXCLUDE_FILES:
                continue

            full_path = os.path.join(dirpath, filename)
            absolute_path = os.path.abspath(full_path)
            
            if absolute_path in processed_files:
                continue
            processed_files.add(absolute_path)

            relative_path = os.path.relpath(full_path, root_dir).replace(os.sep, '/')
            
            header = f"--- START OF FILE: {relative_path} ---\n"
            footer = f"--- END OF FILE: {relative_path} ---\n\n"
            content = ""
            is_sample = False

            try:
                if file_ext == ".py":
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                elif file_ext in SAMPLE_EXTENSIONS:
                    is_sample = True
                    lines = []
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for i, line in enumerate(f):
                            if i >= MAX_SAMPLE_LINES:
                                break
                            lines.append(line)
                    content = "".join(lines)
                    if len(lines) >= MAX_SAMPLE_LINES:
                         header = f"--- START OF FILE: {relative_path} (First {MAX_SAMPLE_LINES} lines) ---\n"
                else:
                    continue 
                    
                combined_content.append(header)
                combined_content.append(content)
                combined_content.append(footer)

            except FileNotFoundError:
                 combined_content.append(f"--- ERROR: Could not find file {relative_path} during processing ---\n\n")
            except Exception as e:
                 combined_content.append(f"--- ERROR reading file {relative_path}: {e} ---\n\n")

    return "".join(combined_content)

if __name__ == "__main__":
    print(f"Starting file combination process in directory: {PROJECT_ROOT_DIR}")
    print(f"Excluding Dirs: {EXCLUDE_DIRS}")
    print(f"Excluding Files/Exts: {EXCLUDE_FILES}")
    print(f"Sampling Exts (up to {MAX_SAMPLE_LINES} lines): {SAMPLE_EXTENSIONS}")
    print("-" * 30)
    
    full_output = combine_files(PROJECT_ROOT_DIR)
    
    # --- Save to File ---
    try:
        output_file_path = os.path.join(PROJECT_ROOT_DIR, OUTPUT_FILENAME)
        with open(output_file_path, 'w', encoding='utf-8') as f:
            f.write(full_output)
        print(f"\nSuccessfully combined content saved to: {output_file_path}") # Confirmation message
    except Exception as e:
        print(f"\nError saving combined content to file '{OUTPUT_FILENAME}': {e}", file=sys.stderr)
    # --- End Save to File ---

    # --- Console printing commented out ---
    # print("\n*** Combined Project Content Start ***\n")
    # print(full_output)
    # print("\n*** Combined Project Content End ***\n")
    # --- End Console printing ---

    print("Process finished.")
    print(f"\n>>> IMPORTANT: Please carefully REVIEW the generated file ('{OUTPUT_FILENAME}') for any sensitive information (API keys, passwords, personal data in logs/samples) before sharing! <<<")--- END OF FILE: _code-concatenator.py ---

--- START OF FILE: functions/data_processor.py ---
# functions/data_processor.py
"""
Functions for loading and preprocessing calendar data.
"""
import pandas as pd
import json
import re
import logging
import os # Added for path operations
from datetime import datetime

# Import config relative to project root (sys.path should be set by main.py)
import config 

# Setup logger for this module
logger = logging.getLogger(__name__)

def parse_datetime(dt_str):
    """Handles different datetime formats found in the JSON."""
    if not dt_str: return pd.NaT # Handle None or empty strings

    if isinstance(dt_str, str):
        # Try ISO 8601 format with timezone first (most robust)
        try:
            # Handles 'YYYY-MM-DD HH:MM:SS+ZZ:ZZ', 'YYYY-MM-DDTHH:MM:SSZ', etc.
            dt = pd.to_datetime(dt_str, errors='coerce', utc=True)
            if pd.notna(dt): return dt
        except Exception:
             pass

        # Handle formats like '20241016T180652Z' (common in iCal)
        try:
            dt = pd.to_datetime(dt_str, format='%Y%m%dT%H%M%SZ', errors='coerce', utc=True)
            if pd.notna(dt): return dt
        except (ValueError, TypeError):
            pass # Try next format

        # Handle vDDDTypes format by extracting the datetime part
        # Regex improved to handle optional T separator and optional timezone Z
        match = re.search(r"(\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}(?:[+-]\d{2}:\d{2}|Z)?)", dt_str)
        if match:
            extracted_str = match.group(1)
            try:
                # Parse the extracted string, trying common variations
                dt = pd.to_datetime(extracted_str, errors='coerce', utc=True)
                if pd.notna(dt): return dt
            except Exception as e:
                 logger.debug(f"Could not parse extracted datetime string '{extracted_str}' from '{dt_str}': {e}")
                 pass # Could not parse extracted string

    elif isinstance(dt_str, dict) and 'dt' in dt_str: # Handle potential nested structure
        # Recursively call parse_datetime on the nested value
        return parse_datetime(dt_str['dt'])

    # If all parsing attempts fail
    logger.warning(f"Could not parse datetime string: {dt_str}")
    return pd.NaT


# Renamed from load_data to match main.py call
def load_raw_calendar_data(source_path=config.RAW_CALENDAR_JSON_PATH) -> pd.DataFrame | None:
    """Loads raw calendar event data from a JSON file, using absolute path."""
    
    # Construct the absolute path for reliable checking and logging
    absolute_source_path = os.path.abspath(source_path)
    
    logger.info(f"Attempting to load raw calendar data from: {absolute_source_path}") # Log absolute path
    
    if not os.path.exists(absolute_source_path): # Check existence using absolute path
        # Log the absolute path it actually checked
        logger.error(f"Error: Raw JSON file not found at the specified path: {absolute_source_path}") 
        return None
    try:
        # Read JSON directly into a pandas DataFrame using the absolute path
        # Handle potential empty file or invalid JSON structure BEFORE reading
        with open(absolute_source_path, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content:
                logger.error(f"Error: JSON file is empty: {absolute_source_path}")
                return None
            # Now try to parse the content
            data = json.loads(content)
            if not isinstance(data, list): # Expecting a list of events
                 logger.error(f"Error: JSON file does not contain a list of events: {absolute_source_path}")
                 return None

        df = pd.DataFrame(data)
        logger.info(f"Successfully loaded data from {absolute_source_path}. Found {len(df)} events.")
        return df

    except json.JSONDecodeError as e:
        logger.error(f"Error: Could not decode JSON from {absolute_source_path}. Invalid JSON: {e}")
        return None
    except ValueError as e:
         logger.error(f"Error reading JSON into DataFrame from {absolute_source_path}. Check format/types. Error: {e}")
         return None
    except Exception as e:
        logger.error(f"An unexpected error occurred loading the data from {absolute_source_path}: {e}")
        return None


def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """Parses timestamps, calculates duration, cleans summary, prepares data."""
    if df is None or df.empty:
        logger.warning("Input DataFrame is empty or None. Skipping preprocessing.")
        return pd.DataFrame(columns=['uid', 'summary', 'start_time', 'end_time', 'duration_minutes', 'duration_hours']) # Define expected output columns

    logger.info(f"Starting preprocessing for {len(df)} events.")

    # Ensure required columns exist for processing time; others are optional
    required_cols = ['start', 'end', 'summary', 'uid']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logger.error(f"Input DataFrame missing required columns for processing: {missing_cols}. Cannot proceed.")
        return pd.DataFrame(columns=df.columns.tolist() + ['start_time', 'end_time', 'duration_minutes', 'duration_hours'])

    df_processed = df.copy()

    # 1. Parse Timestamps using the helper function
    logger.info("Parsing start and end timestamps...")
    df_processed['start_time'] = df_processed['start'].apply(parse_datetime)
    df_processed['end_time'] = df_processed['end'].apply(parse_datetime)

    # Report parsing issues
    start_nulls = df_processed['start_time'].isnull().sum()
    end_nulls = df_processed['end_time'].isnull().sum()
    if start_nulls > 0:
        logger.warning(f"Could not parse 'start' time for {start_nulls}/{len(df_processed)} events.")
    if end_nulls > 0:
        logger.warning(f"Could not parse 'end' time for {end_nulls}/{len(df_processed)} events.")

    # Drop rows where essential time parsing failed (start or end)
    original_count = len(df_processed)
    df_processed.dropna(subset=['start_time', 'end_time'], inplace=True)
    dropped_count = original_count - len(df_processed)
    if dropped_count > 0:
        logger.info(f"Dropped {dropped_count} rows due to missing/unparseable start or end times.")
        if len(df_processed) == 0:
            logger.error("All rows dropped due to time parsing errors. Check date formats in JSON.")
            return pd.DataFrame(columns=df_processed.columns.tolist() + ['duration_minutes', 'duration_hours'])

    # 2. Calculate Duration
    logger.info("Calculating event durations...")
    df_processed['duration'] = df_processed['end_time'] - df_processed['start_time']
    df_processed['duration_minutes'] = df_processed['duration'].dt.total_seconds() / 60.0
    df_processed['duration_hours'] = df_processed['duration_minutes'] / 60.0 # Added duration in hours

    # Filter out negative or zero durations
    original_count = len(df_processed)
    invalid_duration_mask = df_processed['duration_minutes'] <= 0
    invalid_duration_count = invalid_duration_mask.sum()

    if invalid_duration_count > 0:
        logging.warning(f"Found {invalid_duration_count} events with non-positive duration (end <= start).")
        # Optionally log details of invalid events
        # logging.debug("Events with invalid duration:\n" + df_processed[invalid_duration_mask][['uid', 'start_time', 'end_time', 'summary']].to_string())
        df_processed = df_processed[~invalid_duration_mask] # Keep rows where duration > 0
        logging.info(f"Dropped {invalid_duration_count} rows with non-positive duration.")
        if len(df_processed) == 0:
            logging.error("All remaining rows dropped due to non-positive duration.")
            # Return empty df with expected columns
            return pd.DataFrame(columns=df_processed.columns.tolist())

    # 3. Basic Summary Cleaning (as in analysis.py's loading)
    if 'summary' in df_processed.columns:
        df_processed['summary'] = df_processed['summary'].astype(str).fillna('')
        # Could add .str.strip() here as well if needed

    # 4. Select and Rename Columns
    # Keep only columns needed for subsequent steps to simplify DataFrame
    columns_to_keep_final = [
        'uid', 'summary', 'start_time', 'end_time', 
        'duration_minutes', 'duration_hours', 'description', 'location', 'status' 
        # Add 'categories', 'created', etc. if needed for analysis
    ]
    # Ensure only existing columns are selected
    final_cols = [col for col in columns_to_keep_final if col in df_processed.columns]
    df_final = df_processed[final_cols].copy()

    logger.info(f"Preprocessing complete. {len(df_final)} events remaining for analysis.")
    return df_final


def explode_by_physicist(df: pd.DataFrame) -> pd.DataFrame:
    """
    Explodes the DataFrame based on the 'assigned_personnel' list column.
    Renames the column to 'personnel'. Handles potential non-list entries gracefully.
    """
    # Use 'assigned_personnel' which should exist after normalization in main.py
    target_col = 'assigned_personnel' 
    output_col = 'personnel' # Rename to generic 'personnel'

    if target_col not in df.columns:
        logger.error(f"Cannot explode DataFrame: '{target_col}' column not found.")
        # Add the column as 'Unknown' to prevent downstream errors
        df[output_col] = 'Unknown' 
        return df 

    # Ensure the column contains lists, converting non-lists if necessary
    # This handles cases where normalization might return a single string
    def ensure_list(x):
        if isinstance(x, list):
            # If list is empty, represent as 'Unknown'
            return x if x else ['Unknown'] 
        elif pd.isna(x):
            return ["Unknown"] 
        else:
            # Wrap single items (like 'Unknown', 'Unknown_Error', or a single name string) in a list
            return [str(x)] # Ensure it's a string

    df_copy = df.copy() # Work on a copy
    df_copy[target_col] = df_copy[target_col].apply(ensure_list)

    try:
        # Explode the DataFrame
        df_exploded = df_copy.explode(target_col, ignore_index=True) 
        # Rename the exploded column
        df_exploded.rename(columns={target_col: output_col}, inplace=True)
        # Ensure the final 'personnel' column is string type
        df_exploded[output_col] = df_exploded[output_col].fillna('Unknown').astype(str) 
        
        logger.info(f"DataFrame exploded by personnel. Row count changed from {len(df)} to {len(df_exploded)}.")
        return df_exploded
    except Exception as e:
         logger.error(f"Error during DataFrame explosion based on '{target_col}': {e}")
         # Return the original dataframe with the personnel column added as Unknown
         df[output_col] = 'Unknown'
         return df

# Note: save_processed_data and load_processed_data functions 
# were added in a previous refinement step. 
# Ensure they exist below or are imported if placed elsewhere.

def save_processed_data(df, file_path=config.PROCESSED_EVENTS_JSON_PATH):
    """Saves the DataFrame (with extracted personnel) to JSON."""
    logger.info(f"Attempting to save processed data ({len(df)} events) to: {file_path}")
    if df is None or df.empty:
        logger.warning("Attempted to save an empty or None DataFrame. Skipping save.")
        return False
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Convert DataFrame to JSON format (list of records)
        # Handle Timedeltas if present (e.g., 'duration' column if kept)
        df_copy = df.copy()
        if 'duration' in df_copy.columns and pd.api.types.is_timedelta64_dtype(df_copy['duration']):
             df_copy['duration'] = df_copy['duration'].dt.total_seconds()

        # Use pandas to_json with specific date format and handler for non-serializable types
        df_copy.to_json(file_path, orient='records', indent=4, date_format='iso', default_handler=str)
        
        logger.info(f"Successfully saved processed data to {file_path}")
        return True
    except IOError as e:
        logger.error(f"IOError saving processed data to {file_path}: {e}")
        return False
    except Exception as e:
        logger.error(f"An unexpected error occurred during saving processed data: {e}")
        return False

def load_processed_data(file_path=config.PROCESSED_EVENTS_JSON_PATH):
    """Loads the processed data (with personnel) from JSON."""
    logger.info(f"Attempting to load processed data from: {file_path}")
    
    absolute_file_path = os.path.abspath(file_path) # Use absolute path for check/load
    
    if not os.path.exists(absolute_file_path):
        logger.error(f"Error: Processed JSON file not found at {absolute_file_path}")
        return None
    try:
        df = pd.read_json(absolute_file_path, orient='records')
        
        # Convert columns back to appropriate types
        for col in ['start_time', 'end_time']:
             if col in df.columns:
                 df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)
        
        # Convert duration back to Timedelta if saved as seconds
        if 'duration' in df.columns and pd.api.types.is_numeric_dtype(df['duration']):
             df['duration_td'] = pd.to_timedelta(df['duration'], unit='s', errors='coerce')
        
        # Recalculate duration_hours/minutes if needed or missing
        if 'duration_td' in df.columns:
             if 'duration_minutes' not in df.columns:
                 df['duration_minutes'] = df['duration_td'].dt.total_seconds() / 60.0
             if 'duration_hours' not in df.columns:
                 df['duration_hours'] = df['duration_minutes'] / 60.0

        # Ensure 'personnel' (or 'assigned_personnel' if explode failed) column exists
        final_personnel_col = 'personnel' if 'personnel' in df.columns else 'assigned_personnel'
        if final_personnel_col in df.columns:
            df[final_personnel_col] = df[final_personnel_col].fillna('Unknown').astype(str)
        else:
            logger.warning(f"Loaded processed data is missing the '{final_personnel_col}' column.")
            df['personnel'] = 'Unknown' # Add a default column if missing

        logger.info(f"Successfully loaded {len(df)} processed events from {absolute_file_path}")
        return df
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from {absolute_file_path}: {e}")
        return None
    except ValueError as e:
         logger.error(f"Error reading JSON into DataFrame from {absolute_file_path}. Check format/types. Error: {e}")
         return None
    except Exception as e:
        logger.error(f"An unexpected error occurred during loading processed data: {e}")
        return None--- END OF FILE: functions/data_processor.py ---

--- START OF FILE: functions/visualization.py ---
# visualization.py
"""
Generates plots for workload analysis.
"""
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def plot_workload_duration(workload_df: pd.DataFrame, save_path: str | None = None):
    """Generates and shows/saves a bar plot of total duration per physicist."""
    if workload_df is None or workload_df.empty:
        logging.warning("Workload data is empty. Skipping duration plot.")
        return

    if 'total_duration_hours' not in workload_df.columns or 'physicist' not in workload_df.columns:
        logging.error("Workload DataFrame missing required columns for duration plot.")
        return

    plt.figure(figsize=(12, max(6, len(workload_df) * 0.4))) # Adjust height based on number of physicists
    sns.barplot(data=workload_df.sort_values('total_duration_hours', ascending=False),
                x='total_duration_hours', y='physicist', palette='viridis')
    plt.title('Total Assigned Clinical Duration per Physicist (Hours)')
    plt.xlabel('Total Duration (Hours)')
    plt.ylabel('Physicist')
    plt.tight_layout()
    if save_path:
        try:
            plt.savefig(f"{save_path}_duration.png", dpi=300)
            logging.info(f"Duration plot saved to {save_path}_duration.png")
        except Exception as e:
            logging.error(f"Failed to save duration plot: {e}")
    else:
        plt.show()
    plt.close() # Close the plot figure


def plot_workload_events(workload_df: pd.DataFrame, save_path: str | None = None):
    """Generates and shows/saves a bar plot of total events per physicist."""
    if workload_df is None or workload_df.empty:
        logging.warning("Workload data is empty. Skipping event count plot.")
        return

    if 'total_events' not in workload_df.columns or 'physicist' not in workload_df.columns:
         logging.error("Workload DataFrame missing required columns for event count plot.")
         return

    plt.figure(figsize=(12, max(6, len(workload_df) * 0.4))) # Adjust height
    sns.barplot(data=workload_df.sort_values('total_events', ascending=False),
                x='total_events', y='physicist', palette='magma')
    plt.title('Total Number of Assigned Events per Physicist')
    plt.xlabel('Number of Events')
    plt.ylabel('Physicist')
    plt.tight_layout()
    if save_path:
        try:
            plt.savefig(f"{save_path}_events.png", dpi=300)
            logging.info(f"Event count plot saved to {save_path}_events.png")
        except Exception as e:
            logging.error(f"Failed to save event count plot: {e}")
    else:
        plt.show()
    plt.close() # Close the plot figure--- END OF FILE: functions/visualization.py ---

--- START OF FILE: functions/llm_extractor.py ---
# llm_extractor.py
import logging
import json
from json.decoder import JSONDecodeError
import config
import ollama # Ensure ollama is imported if client setup is here

# Configure logging (ensure it's set up)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- LLM Client Setup --- (Keep your existing setup here)
llm_client = None
if config.LLM_PROVIDER == "ollama":
    try:
        import ollama
        llm_client = ollama.Client(host=config.OLLAMA_BASE_URL)
        # Optional: Connection check can go here
        # llm_client.list()
        # logging.info(f"Successfully connected...")
    except ImportError:
        logging.error("Ollama library not installed. Please install with 'pip install ollama'")
        llm_client = None # Ensure it's None on error
    except Exception as e:
        logging.error(f"Error initializing Ollama client: {e}")
        llm_client = None # Ensure it's None on error
else:
    logging.warning(f"LLM_PROVIDER is set to '{config.LLM_PROVIDER}', not 'ollama'. No Ollama client created.")
    llm_client = None


# --- Extraction Function (UPDATED) ---

def extract_physicists_llm(summary: str) -> list[str]:
    """
    Uses the configured Ollama LLM to extract canonical physicist names from a summary string,
    handling various JSON output formats from the LLM.

    Args:
        summary: The calendar event summary string.

    Returns:
        A list of identified canonical physicist names, or ["Unknown"] if none are found
        or an error occurs.
    """
    if not llm_client:
        # Log error only once or less frequently if needed to avoid spamming logs
        # logging.error(f"Ollama client not initialized (host: {config.OLLAMA_BASE_URL}). Cannot extract names.")
        return ["Unknown"]
    if not summary or not isinstance(summary, str):
        logging.debug("Skipping extraction for empty or non-string summary.")
        return ["Unknown"]

    # Construct the prompt (Consider minor tweaks for clarity)
    prompt = f"""
    Your task is to identify physicist names from the provided calendar event summary.
    You are given a specific list of known canonical physicist names.
    Analyze the summary and identify ONLY the canonical names from the list below that are mentioned or clearly referenced in the summary.
    Consider variations in names (e.g., initials, last names only) but map them back to a name present in the canonical list.
    Do not guess or include names not on the list. If multiple physicists are mentioned, include all of them.
    If no physicists from the list are clearly identified, return an empty list.

    Known Canonical Physicist Names:
    {json.dumps(config.CANONICAL_NAMES, indent=2)}

    Event Summary:
    "{summary}"

    Respond ONLY with a valid JSON structure containing the identified canonical names.
    The ideal response is a JSON list like ["Name1", "Name2"] or [].
    If you must use a dictionary, use a key like "names" for the list, e.g., {{"names": ["Name1", "Name2"]}}.
    Do not add explanations or surrounding text.
    """

    extracted_names = [] # Initialize empty list

    try:
        response = llm_client.chat(
            model=config.LLM_MODEL,
            messages=[
                # System prompt can sometimes help reinforce instructions
                {"role": "system", "content": "You are an assistant that extracts specific names from text based on a provided list and outputs ONLY valid JSON containing those names."},
                {"role": "user", "content": prompt}
            ],
            format='json', # Request JSON format
            options={'temperature': 0.1} # Keep temperature low
        )

        content = response.get('message', {}).get('content', '')
        if not content:
             logging.warning(f"LLM returned empty content for summary: '{summary}'")
             return ["Unknown"] # Treat empty content as failure

        # --- Enhanced JSON Parsing Logic ---
        try:
            extracted_data = json.loads(content)

            # 1. Ideal case: Is it directly a list?
            if isinstance(extracted_data, list):
                extracted_names = extracted_data
                logging.debug(f"LLM returned a direct list for '{summary}'")

            # 2. Common case: Is it a dictionary containing a list?
            elif isinstance(extracted_data, dict):
                logging.debug(f"LLM returned dict for '{summary}': {content}")
                found_list = None
                # Check common keys first
                possible_keys = ['names', 'physicists', 'identified_names', 'canonical_names', 'identifiedCanonicalNames', 'result', 'output']
                for key in possible_keys:
                    if key in extracted_data and isinstance(extracted_data.get(key), list):
                        found_list = extracted_data[key]
                        logging.debug(f"Found list under key '{key}'")
                        break # Found it

                # Fallback: If no known key worked, check *any* value that is a list
                if found_list is None:
                    for value in extracted_data.values():
                        if isinstance(value, list):
                            found_list = value
                            logging.debug("Found list as a dictionary value (unknown key)")
                            break # Use the first list found

                # Fallback: Handle the "{'Name1': true, 'Name2': true}" case
                if found_list is None:
                     # Check if all values are boolean (or maybe just True)
                     all_bools = all(isinstance(v, bool) for v in extracted_data.values())
                     # Or specifically check for True if that's the pattern
                     # all_true = all(v is True for v in extracted_data.values())
                     if extracted_data and all_bools: # Ensure dict not empty
                         found_list = list(extracted_data.keys()) # Assume keys are the names
                         logging.debug("Found names as keys in a boolean dictionary")

                if found_list is not None:
                    extracted_names = found_list
                else:
                    # If it's a dictionary but we couldn't find a list or expected structure
                    logging.warning(f"LLM returned dict, but failed to extract expected list structure: {content}")
                    # Keep extracted_names as []

            # 3. Handle unexpected format
            else:
                 logging.warning(f"LLM (JSON mode) returned unexpected format (not list or dict): {content}")
                 # Keep extracted_names as []

        except JSONDecodeError:
            logging.error(f"Failed to decode JSON response from LLM (model: {config.LLM_MODEL}): {content}")
            return ["Unknown"] # Treat JSON decode error as failure
        except Exception as e:
             logging.error(f"Error processing LLM response content structure: {e}\nResponse content: {content}")
             return ["Unknown"] # Treat other parsing errors as failure

        # --- Validation ---
        # Ensure extracted names are strings before validation
        validated_names = [str(name) for name in extracted_names if str(name) in config.CANONICAL_NAMES]

        if not validated_names:
            if not extracted_names:
                 # LLM correctly identified no names OR parsing failed to find any list
                 logging.debug(f"LLM/Parsing found no known physicists in: '{summary}' (Raw response: {content})")
            else:
                 # LLM returned names, but none matched the canonical list after filtering
                 logging.warning(f"LLM (model: {config.LLM_MODEL}) returned names not in canonical list for '{summary}': {extracted_names}. Filtered. (Raw: {content})")
            # Return ["Unknown"] if validation is empty, simplifying downstream logic
            return ["Unknown"]
        else:
            logging.debug(f"LLM (model: {config.LLM_MODEL}) successfully identified & validated {validated_names} in '{summary}'")
            return validated_names

    except Exception as e:
        # Catch Ollama client errors / network errors etc.
        logging.error(f"Error calling Ollama API (model: {config.LLM_MODEL}, host: {config.OLLAMA_BASE_URL}) for summary '{summary}': {e}")
        # Consider adding more specific error handling (e.g., connection errors) if needed
        return ["Unknown"]--- END OF FILE: functions/llm_extractor.py ---

--- START OF FILE: functions/analysis.py ---
import json
import pandas as pd
import re
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# --- Configuration ---
JSON_FILE_PATH = 'calendar.json'

PHYSICISTS_CONFIG = {
    "A. Alexandrian": {"clinical_pct": 0.80, "variations": ["A. Alexandrian", "Alexandrian"]},
    "D. Perrin": {"clinical_pct": 0.95, "variations": ["D. Perrin", "Perrin"]},
    "D. Neck": {"clinical_pct": 0.95, "variations": ["D. Neck", "Neck"]},
    "D. Solis": {"clinical_pct": 0.60, "variations": ["D. Solis", "Solis"]},
    "C. Chu": {"clinical_pct": 0.95, "variations": ["C. Chu", "Chu"]},
    "B. Smith": {"clinical_pct": 0.95, "variations": ["B. Smith", "Smith"]}, # Ambiguous if just "Smith"
    "A. McGuffey": {"clinical_pct": 0.80, "variations": ["A. McGuffey", "McGuffey"]},
    "E. Chorniak": {"clinical_pct": 0.95, "variations": ["E. Chorniak", "Chorniak"]},
    "G. Pitcher": {"clinical_pct": 0.50, "variations": ["G. Pitcher", "Pitcher"]},
    "S. Stathakis": {"clinical_pct": 0.40, "variations": ["S. Stathakis", "Stathakis"]},
    "C. Schneider": {"clinical_pct": 0.80, "variations": ["C. Schneider", "Schneider"]},
    "R. Guidry": {"clinical_pct": 0.95, "variations": ["R. Guidry", "Guidry"]},
    "J. Voss": {"clinical_pct": 0.95, "variations": ["J. Voss", "Voss"]},
    "A. Husain": {"clinical_pct": 0.95, "variations": ["A. Husain", "Husain"]}, # Estimate clinical %
    "J. Chen": {"clinical_pct": 0.95, "variations": ["J. Chen", "Chen"]},       # Estimate clinical %
# Re-generate VARIATION_MAP automatically in the script
    # Add other known names if applicable (e.g., from the summaries)
    "Wood": {"clinical_pct": 0.95, "variations": ["Wood"]}, # Assuming Wood is ~95% clinical
    "King": {"clinical_pct": 0.95, "variations": ["King", "King III"]}, # Assuming King is ~95% clinical
    "Kovtun": {"clinical_pct": 0.95, "variations": ["Kovtun"]}, # Assuming Kovtun is ~95% clinical
    "Wang": {"clinical_pct": 0.95, "variations": ["Wang"]}, # Assuming Wang is ~95% clinical
    "Elson": {"clinical_pct": 0.95, "variations": ["Elson"]}, # Assuming Elson is ~95% clinical
    "Castle": {"clinical_pct": 0.95, "variations": ["Castle"]}, # Assuming Castle is ~95% clinical
    "Hymel": {"clinical_pct": 0.95, "variations": ["Hymel"]}, # Assuming Hymel is ~95% clinical
    "Henkelmann": {"clinical_pct": 0.95, "variations": ["Henkelmann"]},# Assuming Henkelmann is ~95% clinical
    "Bermudez": {"clinical_pct": 0.95, "variations": ["Bermudez"]},# Assuming Bermudez is ~95% clinical
    # Example of how to handle someone not explicitly listed initially
    # "T. Hagan": {"clinical_pct": 0.95, "variations": ["T. Hagan", "Hagan"]},
    # "G. Debevec": {"clinical_pct": 0.95, "variations": ["G. Debevec", "Debevec"]}

}

# Build a reverse map from variation to canonical name
VARIATION_MAP = {}
for name, config in PHYSICISTS_CONFIG.items():
    for var in config["variations"]:
        VARIATION_MAP[var.lower()] = name # Use lower case for matching

# --- Helper Functions ---

def parse_datetime(dt_str):
    """Handles different datetime formats found in the JSON."""
    if isinstance(dt_str, str):
        # Handle formats like '2024-10-16 18:00:00+00:00'
        try:
            return pd.to_datetime(dt_str, errors='coerce')
        except Exception:
            # Handle formats like '20241016T180652Z'
            try:
                 return pd.to_datetime(dt_str, format='%Y%m%dT%H%M%SZ', errors='coerce', utc=True)
            except Exception:
                 # Handle vDDDTypes format by extracting the datetime part
                 match = re.search(r"(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})", dt_str)
                 if match:
                     try:
                        return pd.to_datetime(match.group(1), errors='coerce', utc=True)
                     except Exception:
                        return pd.NaT
                 else:
                     return pd.NaT

    elif isinstance(dt_str, dict) and 'dt' in dt_str: # Handle potential nested structure if exists
         return parse_datetime(dt_str['dt'])
    return pd.NaT


def extract_physicists(summary):
    """Extracts physicist names/initials from the summary string."""
    found_physicists = set()
    
    # Split by '//' and check the last few segments
    segments = [s.strip() for s in summary.split('//') if s.strip()]
    
    # Check the last 3 segments (most likely place for names)
    segments_to_check = segments[-3:] 
    
    for segment in segments_to_check:
        # Clean up potential noise like '?'
        cleaned_segment = segment.replace('?', '').strip()
        
        # Attempt direct match using the variation map
        if cleaned_segment.lower() in VARIATION_MAP:
            found_physicists.add(VARIATION_MAP[cleaned_segment.lower()])
            continue # Found a match, move to next segment if any

        # Try matching initials like "A. Alexandrian" or single last names
        # Be careful with single initials as they are ambiguous
        potential_names = re.findall(r'([A-Z]\.\s*[A-Z][a-zA-Z]+(?:\s*[A-Z][a-zA-Z]+)?|[A-Z][a-zA-Z]+(?:(?:\s|,|&)[A-Z][a-zA-Z]+)*)', cleaned_segment)
        # Example potential_names for "Kovtun // D. Perrin" -> ['Kovtun', 'D. Perrin']
        
        for name in potential_names:
            name = name.strip()
            if name.lower() in VARIATION_MAP:
                 found_physicists.add(VARIATION_MAP[name.lower()])
            # Handle cases like "King III" -> map to "King"
            elif name.replace(' III', '').lower() in VARIATION_MAP:
                 found_physicists.add(VARIATION_MAP[name.replace(' III', '').lower()])


    if not found_physicists:
        # Fallback: Check the entire summary if segments failed
         for var, canonical_name in VARIATION_MAP.items():
              # Use regex to find variations as whole words to avoid partial matches
              # (e.g., finding 'King' in 'Parking')
              if re.search(r'\b' + re.escape(var) + r'\b', summary, re.IGNORECASE):
                   found_physicists.add(canonical_name)
                   
    return list(found_physicists) if found_physicists else ["Unknown"]

# --- Main Processing ---

# 1. Load Data
try:
    with open(JSON_FILE_PATH, 'r') as f:
        data = json.load(f)
    df = pd.DataFrame(data)
except FileNotFoundError:
    print(f"Error: JSON file not found at {JSON_FILE_PATH}")
    exit()
except json.JSONDecodeError:
    print(f"Error: Could not decode JSON from {JSON_FILE_PATH}")
    exit()
except Exception as e:
    print(f"An unexpected error occurred loading the data: {e}")
    exit()

# 2. Parse Timestamps and Calculate Duration
df['start_time'] = df['start'].apply(parse_datetime)
df['end_time'] = df['end'].apply(parse_datetime)

# Drop rows where time parsing failed
df.dropna(subset=['start_time', 'end_time'], inplace=True)

# Ensure timezone consistency (convert all to UTC for calculation)
df['start_time'] = df['start_time'].dt.tz_convert('UTC')
df['end_time'] = df['end_time'].dt.tz_convert('UTC')


df['duration_minutes'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 60

# Filter out negative or zero durations which indicate errors
df = df[df['duration_minutes'] > 0]

# 3. Extract Physicists
df['assigned_physicists'] = df['summary'].apply(extract_physicists)

# 4. Explode DataFrame for multi-physicist events
#    Each row with multiple physicists becomes multiple rows, one for each.
#    This allows easy grouping but means durations are counted per physicist involved.
df_exploded = df.explode('assigned_physicists')
df_exploded.rename(columns={'assigned_physicists': 'physicist'}, inplace=True)

# --- Analysis ---

# Filter out 'Unknown' assignments for workload analysis
df_assigned = df_exploded[df_exploded['physicist'] != "Unknown"].copy()

# Calculate workload metrics
workload = df_assigned.groupby('physicist').agg(
    total_events=('uid', 'count'),
    total_duration_minutes=('duration_minutes', 'sum')
).reset_index()

# Convert total duration to hours for readability
workload['total_duration_hours'] = workload['total_duration_minutes'] / 60

# Add clinical percentage context
workload['clinical_pct'] = workload['physicist'].map(lambda name: PHYSICISTS_CONFIG.get(name, {}).get('clinical_pct', None))

# Calculate expected clinical hours (based on total hours in analysis period - ROUGH ESTIMATE)
# NOTE: This is a VERY rough estimate. A proper calculation needs the *total* working hours
# for the period covered by the calendar data, which isn't available here.
# Let's assume a standard work week for simplicity in demonstrating the concept.
total_analysis_duration_days = (df['end_time'].max() - df['start_time'].min()).days
if total_analysis_duration_days == 0: total_analysis_duration_days = 1 # Avoid division by zero for short periods
# Assuming ~8 hours/day clinical availability for a 100% clinical person over the period
# This needs significant refinement based on actual work patterns.
# For now, let's just show the assigned hours vs clinical %
# estimated_total_available_hours = total_analysis_duration_days * 8
# workload['expected_clinical_hours'] = workload['clinical_pct'] * estimated_total_available_hours
# workload['workload_vs_expected(%)'] = (workload['total_duration_hours'] / workload['expected_clinical_hours']) * 100

workload = workload.sort_values(by='total_duration_hours', ascending=False)


# --- Output ---
print("--- Physicist Workload Summary ---")
print(workload[['physicist', 'clinical_pct', 'total_events', 'total_duration_hours']].round(2))

print("\n--- Events Assigned to 'Unknown' ---")
unknown_events = df_exploded[df_exploded['physicist'] == "Unknown"]
if not unknown_events.empty:
    print(f"Found {len(unknown_events)} events with unknown physicist assignment.")
    print(unknown_events[['uid', 'summary']].head()) # Print first few
else:
    print("No events found with unknown physicist assignment.")

# --- Visualization ---
plt.figure(figsize=(12, 8))
sns.barplot(data=workload, x='total_duration_hours', y='physicist', palette='viridis')
plt.title('Total Assigned Clinical Duration per Physicist (Hours)')
plt.xlabel('Total Duration (Hours)')
plt.ylabel('Physicist')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 8))
sns.barplot(data=workload.sort_values('total_events', ascending=False), x='total_events', y='physicist', palette='magma')
plt.title('Total Number of Assigned Events per Physicist')
plt.xlabel('Number of Events')
plt.ylabel('Physicist')
plt.tight_layout()
plt.show()

# Optional: Plot workload relative to clinical % (if a good baseline is established)
# This requires a meaningful way to calculate 'expected hours'.
# Example (needs refinement on expected hours calculation):
# workload_filtered = workload.dropna(subset=['expected_clinical_hours'])
# if not workload_filtered.empty:
#     plt.figure(figsize=(12, 8))
#     sns.barplot(data=workload_filtered.sort_values('workload_vs_expected(%)', ascending=False),
#                 x='workload_vs_expected(%)', y='physicist', palette='coolwarm')
#     plt.title('Assigned Workload vs. Expected Clinical Availability (%)')
#     plt.xlabel('Workload / Expected (%)')
#     plt.ylabel('Physicist')
#     plt.axvline(100, color='grey', linestyle='--', label='100% Expected')
#     plt.legend()
#     plt.tight_layout()
#     plt.show()--- END OF FILE: functions/analysis.py ---

--- START OF FILE: data/calendar.json (First 100 lines) ---
[
    {
        "uid": "0xec",
        "summary": "SpaceOAR VUE - Wilbert Peterson (A235174)/Wood/RN 6am/Insert 7am/RTP/8am",
        "start": "2023-08-07 12:00:00+00:00",
        "end": "2023-08-07 13:00:00+00:00",
        "description": "",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f4dd0>",
        "created": "vDDDTypes(2023-06-15 19:14:00+00:00, Parameters({}))",
        "last_modified": "20230615T191400Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x10c",
        "summary": "GK Tx SBRT 1:5 : John Estess (A236273) // Tx @ 8 am // King // D. Solis",
        "start": "2023-08-07 13:00:00+00:00",
        "end": "2023-08-07 14:00:00+00:00",
        "description": " \n\n",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f5250>",
        "created": "vDDDTypes(2023-08-02 20:03:18+00:00, Parameters({}))",
        "last_modified": "20230807T221811Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x12c",
        "summary": "WH HDR 5:5 Kathleen Davis (A236065) // Tx @ 830 am // Castle //D. Neck",
        "start": "2023-08-07 13:30:00+00:00",
        "end": "2023-08-07 14:30:00+00:00",
        "description": " \n\n",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f4aa0>",
        "created": "vDDDTypes(2023-07-11 19:36:31+00:00, Parameters({}))",
        "last_modified": "20230807T221811Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x14c",
        "summary": "Fugarino, Anthony (A235973)/SpaceOAR Vue/Wood/RN 11am/Insert 12pm/RTP 1pm",
        "start": "2023-08-07 17:00:00+00:00",
        "end": "2023-08-07 18:00:00+00:00",
        "description": "",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f4ef0>",
        "created": "vDDDTypes(2023-07-05 16:31:26+00:00, Parameters({}))",
        "last_modified": "20230706T152302Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x16c",
        "summary": "WH HDR CYL 2:5: Joyce Brooks (A236526) // Tx @ 1230 pm // Castle // G. Pitcher // E. Chorniak",
        "start": "2023-08-07 17:30:00+00:00",
        "end": "2023-08-07 18:30:00+00:00",
        "description": " \n\n",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f53d0>",
        "created": "vDDDTypes(2023-07-18 19:52:02+00:00, Parameters({}))",
        "last_modified": "20230807T221810Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x194",
        "summary": "GK Tx: Alexis Clark (A231666) // Tx @ 1 pm // Wood // D. Solis",
        "start": "2023-08-07 18:00:00+00:00",
        "end": "2023-08-07 20:00:00+00:00",
        "description": "Pt: Alexis M. Clark\n\nDOB: 6/26/1995  \n\nMR# A231666\n\nDx: Brain Mets \n\nDr. Wood \n\n \n\n7/28/23\n\n*\tFit Old Mask 12:00 PM\n*\tMRI @ 12:30 PM\n\n \n\n7/31/23\n\n*\tFollow-up w/ Dr  @ 1:00 PM\n\n \n\n*\t*No Meds prior*\n\n \n\n08/07/2023  \n\n*\tFrameless GK Treat x 1 fx @ 1:00 PM \n\n \n\n",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f4b00>",
        "created": "vDDDTypes(2023-08-07 02:07:09+00:00, Parameters({}))",
        "last_modified": "20230807T142317Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x1b4",
        "summary": "Post GK: Patricia Allen (A223812) // Fit old mask @ 2 pm // MRI @ 230 pm // Wood // C. Chu",
        "start": "2023-08-07 19:00:00+00:00",
        "end": "2023-08-07 20:00:00+00:00",
        "description": " \n\n",
        "location": "",
        "status": "CONFIRMED",
        "categories": "<icalendar.prop.vCategory object at 0x14fd5c6f4fe0>",
        "created": "vDDDTypes(2023-06-21 19:59:54+00:00, Parameters({}))",
        "last_modified": "20230807T221810Z",
        "dtstamp": "vDDDTypes(2025-04-06 00:44:35+00:00, Parameters({}))"
    },
    {
        "uid": "0x1d4",
        "summary": "WH HDR CYL 2:2: Trisha Cote (A232613) // Tx @ 330 pm // Castle // G. Pitcher // E. Chorniak",
        "start": "2023-08-07 20:30:00+00:00",
        "end": "2023-08-07 21:30:00+00:00",
        "description": " \n\n",
        "location": "",
        "status": "CONFIRMED",
--- END OF FILE: data/calendar.json ---

--- START OF FILE: app/config.py ---
# app/config.py 
import os
import logging
import sys # Needed for path manipulation if run directly

# --- Core Path Configuration ---
# Define the project root directory dynamically. 
# Since this config.py is inside 'app', we need to go up one level.
APP_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(APP_DIR) 

# Add project root to path if it's not already there 
# (helps if modules try to import config directly)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# Define output and data directories relative to the project root
OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'output')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data') # Correctly identify data directory

# --- Input/Output File Paths ---
# Point to the correct raw data file inside the DATA_DIR
RAW_CALENDAR_JSON_PATH = os.path.join(DATA_DIR, 'calendar.json') # 

# Define the path for the processed data (output of main.py, input for analysis.py)
PROCESSED_EVENTS_JSON_PATH = os.path.join(OUTPUT_DIR, 'processed_events.json')
# Define the path for the log file within the output directory
LOG_FILE = os.path.join(OUTPUT_DIR, 'calendar_analysis.log') 
# Define the paths for plots within the output directory
PLOT_DURATION_PATH = os.path.join(OUTPUT_DIR, 'workload_duration_by_physicist.png')
PLOT_EVENTS_PATH = os.path.join(OUTPUT_DIR, 'workload_events_by_physicist.png')

# --- Ensure Output Directory Exists ---
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ... (rest of your config remains the same) ...

# --- LLM Configuration ---
# Set to None to disable LLM features completely
LLM_PROVIDER = "ollama" # Options: "ollama", "openai", "anthropic", None
OLLAMA_BASE_URL = "http://192.168.1.5:11434" # Required if LLM_PROVIDER is "ollama"
LLM_MODEL = "llama3.1:8b" # Model name specific to the provider

# --- Performance Configuration ---
# Number of parallel threads for LLM extraction (adjust based on system/API limits)
LLM_MAX_WORKERS = 10  

# --- Personnel Configuration ---
# Defines personnel involved, their roles, clinical percentages, and name variations.
# 'variations' are used by VARIATION_MAP to normalize names found in event summaries.
PERSONNEL_CONFIG = {
    # Physicists
    "A. Alexandrian": {"role": "physicist", "clinical_pct": 0.80, "variations": ["A. Alexandrian", "Alexandrian"]},
    "D. Perrin": {"role": "physicist", "clinical_pct": 0.95, "variations": ["D. Perrin", "Perrin"]},
    "D. Neck": {"role": "physicist", "clinical_pct": 0.95, "variations": ["D. Neck", "Neck"]},
    "D. Solis": {"role": "physicist", "clinical_pct": 0.60, "variations": ["D. Solis", "Solis"]},
    "C. Chu": {"role": "physicist", "clinical_pct": 0.95, "variations": ["C. Chu", "Chu"]},
    "B. Smith": {"role": "physicist", "clinical_pct": 0.95, "variations": ["B. Smith", "Smith"]}, # Ambiguity if just "Smith"
    "A. McGuffey": {"role": "physicist", "clinical_pct": 0.80, "variations": ["A. McGuffey", "McGuffey"]},
    "E. Chorniak": {"role": "physicist", "clinical_pct": 0.95, "variations": ["E. Chorniak", "Chorniak"]},
    "G. Pitcher": {"role": "physicist", "clinical_pct": 0.50, "variations": ["G. Pitcher", "Pitcher"]},
    "S. Stathakis": {"role": "physicist", "clinical_pct": 0.40, "variations": ["S. Stathakis", "Stathakis"]},
    "C. Schneider": {"role": "physicist", "clinical_pct": 0.80, "variations": ["C. Schneider", "Schneider"]},
    "R. Guidry": {"role": "physicist", "clinical_pct": 0.95, "variations": ["R. Guidry", "Guidry"]},
    "J. Voss": {"role": "physicist", "clinical_pct": 0.95, "variations": ["J. Voss", "Voss"]},
    "A. Husain": {"role": "physicist", "clinical_pct": 0.95, "variations": ["A. Husain", "Husain"]},
    "J. Chen": {"role": "physicist", "clinical_pct": 0.95, "variations": ["J. Chen", "Chen"]},
    # Physicians (Identified based on previous analysis/user input)
    "Wood": {"role": "physician", "clinical_pct": 0.95, "variations": ["Wood"]}, # clinical_pct might represent availability/FTE here
    "King": {"role": "physician", "clinical_pct": 0.95, "variations": ["King", "King III"]},
    "Kovtun": {"role": "physician", "clinical_pct": 0.95, "variations": ["Kovtun"]}, 
    "Wang": {"role": "physician", "clinical_pct": 0.95, "variations": ["Wang"]},
    "Elson": {"role": "physician", "clinical_pct": 0.95, "variations": ["Elson"]},
    "Castle": {"role": "physician", "clinical_pct": 0.95, "variations": ["Castle"]}, 
    "Hymel": {"role": "physician", "clinical_pct": 0.95, "variations": ["Hymel"]},
    "Henkelmann": {"role": "physician", "clinical_pct": 0.95, "variations": ["Henkelmann"]},
    "Bermudez": {"role": "physician", "clinical_pct": 0.95, "variations": ["Bermudez"]},
    # Example of non-clinical personnel (if needed)
    # "Admin Staff": {"role": "admin", "clinical_pct": 0.0, "variations": ["Admin", "Support"]},
}

# --- Derived Personnel Configuration (Do Not Modify Manually) ---

# Map all variations (lowercase) to the canonical personnel name for easy lookup
VARIATION_MAP = {}
for canonical_name, config_data in PERSONNEL_CONFIG.items():
    if isinstance(config_data, dict) and "variations" in config_data:
        for variation in config_data["variations"]:
            lower_var = variation.lower()
            if lower_var in VARIATION_MAP:
                # Log a warning only if the mapping changes, indicating a potential conflict
                if VARIATION_MAP[lower_var] != canonical_name:
                     logging.warning(
                        f"Duplicate variation '{variation}' (lowercase: '{lower_var}') mapped to '{canonical_name}'. "
                        f"It was previously mapped to '{VARIATION_MAP[lower_var]}'. Overwriting."
                    )
            VARIATION_MAP[lower_var] = canonical_name
    else:
        logging.warning(f"Personnel config for '{canonical_name}' is invalid or missing 'variations'. Skipping.")

# List of canonical names for validation or iteration
CANONICAL_NAMES = list(PERSONNEL_CONFIG.keys())

# Functions to safely access personnel data
def get_personnel_config(personnel_name):
    """Retrieves the entire config dict for a given canonical personnel name."""
    return PERSONNEL_CONFIG.get(personnel_name, {}) # Return empty dict if not found

def get_clinical_pct(personnel_name):
    """Retrieves the clinical percentage for a given canonical personnel name."""
    return PERSONNEL_CONFIG.get(personnel_name, {}).get('clinical_pct', None) # Default to None

def get_role(personnel_name):
    """Retrieves the role for a given canonical personnel name."""
    return PERSONNEL_CONFIG.get(personnel_name, {}).get('role', 'Unknown') # Default to 'Unknown'

# --- Logging Configuration ---
LOGGING_LEVEL_STR = "INFO" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOGGING_LEVEL = getattr(logging, LOGGING_LEVEL_STR.upper(), logging.INFO) # Convert string to logging level
LOG_FORMAT = '%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'

# --- Analysis Parameters ---
# Define the date range for analysis (inclusive). YYYY-MM-DD format.
# Set to None to analyze all events.
START_DATE = None 
END_DATE = None 

# Minimum event duration in minutes to be considered in analysis (filters out short reminders)
MIN_EVENT_DURATION_MINUTES = 5

# --- Plotting Configuration ---
# PLOT_OUTPUT_PATH is defined in File Paths section
PLOT_TITLE = "Personnel Workload Summary"
PLOT_X_LABEL_HOURS = "Total Duration (Hours)"
PLOT_X_LABEL_EVENTS = "Total Event Count"
PLOT_Y_LABEL = "Personnel"
# Limit the number of personnel shown on bar plots for readability
PLOT_PERSONNEL_LIMIT = 25 --- END OF FILE: app/config.py ---

--- START OF FILE: app/main.py ---
# app/main.py
"""
Main script to run the calendar workload analysis pipeline using Ollama
with parallel LLM extraction.
"""
import logging
import pandas as pd
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys
import os
import time # Added for timing

# --- Path Setup ---
# Dynamically add the project root directory (CALENDAR-ANALYSIS) to the Python path
# This allows imports relative to the root (like 'config' and 'functions')
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- End Path Setup ---

# Import project modules using paths relative to the project root
import config # Should find config.py in the root directory
from functions import data_processor
from functions import llm_extractor
from functions import analysis
from functions import visualization

# --- Configure logging ---
# Ensure log directory exists (config should handle this, but double-check)
os.makedirs(config.OUTPUT_DIR, exist_ok=True)
# Clear existing handlers if any
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(
    level=config.LOGGING_LEVEL, 
    format=config.LOG_FORMAT,
    handlers=[
        logging.FileHandler(config.LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
# --- End Logging Config ---

def run_pipeline():
    """Executes the full analysis pipeline."""
    start_pipeline_time = time.time()
    logger.info("--- Starting Calendar Analysis Pipeline ---")

    # 1. Load Data
    # Use the path defined in the refined config.py
    raw_data_path = config.RAW_CALENDAR_JSON_PATH 
    logger.info(f"Attempting to load raw data from: {raw_data_path}")
    # Use the function defined in the refined data_processor.py
    df = data_processor.load_raw_calendar_data(raw_data_path) 
    if df is None or df.empty:
        logger.error(f"Failed to load data from {raw_data_path}. Exiting.")
        return

    # 2. Preprocess Data (Dates, Duration)
    logger.info("Preprocessing data (parsing dates, calculating duration)...")
    df_processed = data_processor.preprocess_data(df)
    if df_processed is None or df_processed.empty:
        logger.error("Preprocessing failed or resulted in empty DataFrame. Exiting.")
        return

    # --- 3. Extract Assigned Personnel using LLM (PARALLELIZED) ---
    if config.LLM_PROVIDER and config.LLM_PROVIDER != "None":
        logger.info(f"Extracting personnel using LLM ({config.LLM_PROVIDER} - {config.LLM_MODEL}) with {config.LLM_MAX_WORKERS} workers...")

        # Initialize LLM Extractor (using refined data_processor)
        try:
            # LLMExtractor now likely takes config details during init or reads config directly
            llm_extractor_instance = llm_extractor.LLMExtractor() 
            # Check client readiness
            if not llm_extractor_instance.is_ready():
                 logger.error(f"LLM Client ({config.LLM_PROVIDER}) not available or failed to connect. Cannot proceed.")
                 if config.LLM_PROVIDER == "ollama":
                      print(f"\nPlease ensure the Ollama server is running at {config.OLLAMA_BASE_URL} and the model '{config.LLM_MODEL}' is available ('ollama list').")
                 return
        except Exception as e:
            logger.error(f"Failed to initialize LLMExtractor: {e}. Check implementation and config.")
            return

        # Ensure 'summary' column exists
        if 'summary' not in df_processed.columns:
            logger.error("'summary' column not found in processed data. Cannot extract personnel.")
            return
        df_processed['summary'] = df_processed['summary'].fillna('') # Fill NaN again just in case

        summaries = df_processed['summary'].tolist()
        # Initialize results with a default value (e.g., empty string or None)
        results = [""] * len(summaries) 

        start_llm_time = time.time()
        logger.info(f"Submitting {len(summaries)} summaries to LLM extractor...")
        
        try:
            with ThreadPoolExecutor(max_workers=config.LLM_MAX_WORKERS) as executor:
                future_to_index = {
                    executor.submit(llm_extractor_instance.extract_personnel_llm, summary): i 
                    for i, summary in enumerate(summaries)
                }

                for future in tqdm(as_completed(future_to_index), total=len(summaries), desc="LLM Extraction Progress"):
                    index = future_to_index[future]
                    try:
                        result = future.result() # Get result (list of names or error string)
                        # Store the result directly (handle list/string conversion later if needed)
                        results[index] = result if result is not None else "Unknown"
                    except Exception as exc:
                        logger.error(f"Summary index {index} ('{summaries[index][:50]}...') generated an exception: {exc}")
                        results[index] = "Unknown_Error" # Assign specific error string

            llm_time_taken = time.time() - start_llm_time
            logger.info(f"LLM extraction completed in {llm_time_taken:.2f} seconds.")

            # Assign the collected results back to the DataFrame (NEW column name)
            df_processed['extracted_personnel'] = results # Use a distinct name
            
            # --- Refine/Normalize Extracted Names (Optional but Recommended) ---
            logger.info("Normalizing extracted personnel names...")
            # This step assumes llm_extractor returns a *list* of names or "Unknown" / "Unknown_Error"
            # We need a function to clean this up and map to canonical names
            df_processed['assigned_personnel'] = df_processed['extracted_personnel'].apply(
                llm_extractor.normalize_and_map_personnel 
            )
            # Log normalization results
            logger.info("Personnel Name Normalization Counts:")
            print(df_processed['assigned_personnel'].value_counts().to_string())
            # --- End Normalization ---


        except Exception as e:
            logger.error(f"An error occurred during parallel LLM extraction: {e}")
            return # Exit if parallel extraction fails

    else:
        logger.warning("LLM_PROVIDER is not configured. Skipping LLM extraction.")
        # Handle cases where LLM is skipped - perhaps use regex or rules-based assignment?
        # For now, create a placeholder column if LLM is skipped
        df_processed['assigned_personnel'] = 'Unknown' 

    # --- End of LLM Extraction ---

    # 4. Save Processed Data (with assigned personnel)
    # Use the path from config for processed output
    logger.info(f"Saving processed data with assigned personnel to {config.PROCESSED_EVENTS_JSON_PATH}...")
    save_success = data_processor.save_processed_data(df_processed, config.PROCESSED_EVENTS_JSON_PATH)
    if not save_success:
        logger.error("Failed to save processed data. Analysis might use incomplete data if run separately.")
        # Decide whether to exit or just warn
        # return # Exit if saving fails is safer

    # 5. Run Analysis (using the SAVED processed data)
    # This decouples the extraction from the analysis run
    logger.info("--- Running Analysis Script ---")
    # analysis.py should now load config.PROCESSED_EVENTS_JSON_PATH
    analysis.generate_report() # Assuming analysis.py contains this entry function

    # Note: Visualization is likely called within analysis.generate_report() now

    end_pipeline_time = time.time()
    logger.info(f"--- Pipeline Finished in {end_pipeline_time - start_pipeline_time:.2f} seconds ---")

if __name__ == "__main__":
    # Ensure the script is run from the project root for imports to work as expected
    current_dir = os.path.basename(os.getcwd())
    expected_dir = os.path.basename(project_root)
    if current_dir != expected_dir:
         print(f"Warning: Script is being run from '{current_dir}'.")
         print(f"Please run from the project root directory ('{expected_dir}') for correct imports:")
         print(f"Example: python app/main.py")
         # sys.exit(1) # Optional: Force exit if run from wrong directory

    run_pipeline()--- END OF FILE: app/main.py ---

--- START OF FILE: utils/requirements.txt ---
pandas
matplotlib
seaborn
ollama  # Added
# openai # Removed or commented out
tqdm--- END OF FILE: utils/requirements.txt ---

